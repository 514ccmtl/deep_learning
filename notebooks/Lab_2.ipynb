{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in Multilayer Neural Networks\n",
    "\n",
    "While we will primarily be working with high-level, abstract toolkits like Keras in this course, understanding how backpropagation works is absolutely essential to using neural networks. \n",
    "\n",
    "In this exercise, we will build our own backpropagation algorithm - working through each step, to ensure that we can follow it."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Check if the packages are installed, if not install them.\n",
    "# Note - if you are working locally, you may want to comment this section out\n",
    "# ...and use your preferred method of installing packages.\n",
    "import importlib\n",
    "\n",
    "def install_if_missing(package):\n",
    "    if importlib.util.find_spec(package) is None:\n",
    "        !pip install {package}\n",
    "\n",
    "for package in [\"matplotlib\", \"numpy\", \"sklearn\"]:\n",
    "    install_if_missing(package)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Just like in Lab 1, we'll be working with the MNIST dataset. We will load it and plot an example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Of course, we need to split our data into training and testing sets before we use it, just the same as in Lab 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Implementation\n",
    "\n",
    "## a) Logistic Regression\n",
    "\n",
    "In this section we will implement a logistic regression model trainable with SGD using numpy. Here are the objectives:\n",
    "\n",
    "- Implement a simple forward model with no hidden layer (equivalent to a logistic regression):\n",
    "\n",
    "$y = softmax(\\mathbf{W} \\dot x + b)$\n",
    "\n",
    "- Build a `predict` function which returns the most probable class given an input $x$\n",
    "\n",
    "- Build an accuracy function for a batch of inputs $X$ and the corresponding expected outputs $y_{true}$\n",
    "\n",
    "- Build a grad function which computes $\\frac{d}{dW} -\\log(softmax(W \\dot x + b))$ for an $x$ and its corresponding expected output $y_{true}$ ; check that the gradients are well-defined\n",
    "\n",
    "- Build a train function which uses the grad function output to update $\\mathbf{W}$ and $b$\n",
    "\n",
    "### One-hot encoding for class label data\n",
    "\n",
    "First, let's define a helper function to compute the one hot encoding of an integer array for a fixed number of classes (similar to keras' `to_categorical`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n_classes, y):\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot(n_classes=10, y=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot(n_classes=10, y=[0, 4, 9, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The softmax function\n",
    "\n",
    "Now it's your turn to implement the softmax function. Recall that the softmax function is defined as follows:\n",
    "\n",
    "$$\n",
    "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  e^{x_1}\\\\\\\\\n",
    "  e^{x_2}\\\\\\\\\n",
    "  \\vdots\\\\\\\\\n",
    "  e^{x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If you're not sure how to implement this, take a look at the [numpy documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    # TODO: implement the softmax function\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that this works one vector at a time (and check that the components sum to one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(softmax([10, 2, -3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are using our model to make predictions, we will want to be able to make predictions for multiple samples at once. Let's make sure that our implementation of softmax works for a batch of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way to implement softmax that works both for an individual vector of activations and for a batch of activation vectors at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exp = np.exp(X)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "print(\"softmax of a single vector:\")\n",
    "print(softmax([10, 2, -3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities should sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(softmax([10, 2, -3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sotfmax of 2 vectors:\")\n",
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of probabilities for each input vector of logits should some to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(softmax(X), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that, given the true one-hot encoded class `Y_true` and some predicted probabilities `Y_pred`, returns the negative log likelihood.\n",
    "\n",
    "Recall that the negative log likelihood is defined as follows:\n",
    "\n",
    "$$\n",
    "NLL(Y_{true}, Y_{pred}) = - \\sum_{i=1}^{n}{y_{true, i} \\cdot \\log(y_{pred, i})}\n",
    "$$\n",
    "\n",
    "Note that the `Y_true` and `Y_pred` parameters are numpy arrays, not Python lists. This means that you can use numpy functions to compute the negative log likelihood without using any loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(Y_true, Y_pred):\n",
    "    Y_true = np.asarray(Y_true)\n",
    "    Y_pred = np.asarray(Y_pred)\n",
    "    \n",
    "    # TODO: implement the negative log likelihood\n",
    "    return 0.\n",
    "\n",
    "\n",
    "# Make sure that it works for a simple sample at a time\n",
    "print(nll([1, 0, 0], [.99, 0.01, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see a very high value for this negative log likelihood, since the model is very confident that the third class is the correct one, but the true class is the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nll([1, 0, 0], [0.01, 0.01, .98]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can compute the average negative log likelihood of a group of predictions: `Y_pred` and `Y_true` can therefore be past as 2D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(Y_true, Y_pred):\n",
    "    Y_true = np.atleast_2d(Y_true)\n",
    "    Y_pred = np.atleast_2d(Y_pred)\n",
    "\n",
    "    # TODO: implement the negative log likelihood for a batch of samples\n",
    "    return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the average NLL of the following 3 almost perfect\n",
    "# predictions is close to 0\n",
    "Y_true = np.array([[0, 1, 0],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "Y_pred = np.array([[0,   1,    0],\n",
    "                   [.99, 0.01, 0],\n",
    "                   [0,   0,    1]])\n",
    "\n",
    "print(nll(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the following Logistic Regression model. Have a look at the code and make sure that you understand what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Initialize the weights and biases with random numbers\n",
    "        self.W = np.random.uniform(size=(input_size, output_size),\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.b = np.random.uniform(size=output_size,\n",
    "                                   high=0.1, low=-0.1)\n",
    "        \n",
    "        # Store the input size and output size\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Compute the linear combination of the input and weights\n",
    "        Z = np.dot(X, self.W) + self.b\n",
    "        \n",
    "        # Return the softmax of the linear combination\n",
    "        return softmax(Z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Return the most probable class for each sample in X\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "    \n",
    "    def grad_loss(self, x, y_true):\n",
    "        # Compute the gradient of the loss with respect to W and b for a single sample (x, y_true)\n",
    "        \n",
    "        # Make predictions for x\n",
    "        y_pred = self.forward(x)\n",
    "        \n",
    "        # Compute the gradient of the loss\n",
    "        dnll_output =  y_pred - one_hot(self.output_size, y_true)\n",
    "        \n",
    "        # Compute the gradient of the loss with respect to W\n",
    "        grad_W = np.outer(x, dnll_output)\n",
    "        \n",
    "        # Compute the gradient of the loss with respect to b\n",
    "        grad_b = dnll_output\n",
    "        \n",
    "        # Return the gradient for W and b\n",
    "        grads = {\"W\": grad_W, \"b\": grad_b}\n",
    "        return grads\n",
    "    \n",
    "    def train(self, x, y, learning_rate):\n",
    "        # Traditional SGD update without momentum\n",
    "        \n",
    "        # Compute the gradient for the sample\n",
    "        grads = self.grad_loss(x, y)\n",
    "        \n",
    "        # Update the weights\n",
    "        self.W = self.W - learning_rate * grads[\"W\"]\n",
    "        \n",
    "        # Update the biases\n",
    "        self.b = self.b - learning_rate * grads[\"b\"]      \n",
    "        \n",
    "    def loss(self, X, y):\n",
    "        # Compute the average negative log likelihood over the whole training set\n",
    "        return nll(one_hot(self.output_size, y), self.forward(X))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        # Compute the accuracy of the model on the training set\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model and test its forward inference\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = len(np.unique(y_train))\n",
    "lr = LogisticRegression(n_features, n_classes)\n",
    "\n",
    "print(\"Evaluation of the untrained model:\")\n",
    "train_loss = lr.loss(X_train, y_train)\n",
    "train_acc = lr.accuracy(X_train, y_train)\n",
    "test_acc = lr.accuracy(X_test, y_test)\n",
    "\n",
    "print(f'Initial train loss: {train_loss:.4f}, train acc: {train_acc:.3f}, test acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the model on an example, visualizing the prediction probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(model, sample_idx=0, classes=range(10)):\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "    ax0.imshow(X_test[sample_idx:sample_idx+1].reshape(8, 8),\n",
    "               cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax0.set_title(\"True image label: %d\" % y_test[sample_idx]);\n",
    "\n",
    "\n",
    "    ax1.bar(classes, one_hot(len(classes), y_test[sample_idx]), label='true')\n",
    "    ax1.bar(classes, model.forward(X_test[sample_idx]), label='prediction', color=\"red\")\n",
    "    ax1.set_xticks(classes)\n",
    "    prediction = model.predict(X_test[sample_idx])\n",
    "    ax1.set_title('Output probabilities (prediction: %d)'\n",
    "                  % prediction)\n",
    "    ax1.set_xlabel('Digit class')\n",
    "    ax1.legend()\n",
    "    \n",
    "plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now it's time to start training! We will train for a single epoch, and then evaluate the model on the training and testing sets:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for one epoch\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "    lr.train(x, y, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        train_loss = lr.loss(X_train, y_train)\n",
    "        train_acc = lr.accuracy(X_train, y_train)\n",
    "        test_acc = lr.accuracy(X_test, y_test)\n",
    "        print(f'Iteration {i}, train loss: {train_loss:.4f}, train acc: {train_acc:.3f}, test acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Feedforward Multilayer\n",
    "\n",
    "The objective of this section is to implement the backpropagation algorithm (SGD with the chain rule) on a single layer neural network using the sigmoid activation function.\n",
    "\n",
    "- Implement the `sigmoid` and its element-wise derivative `dsigmoid` functions:\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dsigmoid(x) = sigmoid(x) \\cdot (1 - sigmoid(x))\n",
    "$$\n",
    "\n",
    "Remember that you can use your `sigmoid` function inside your `dsigmoid` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    # TODO: implement the sigmoid function\n",
    "    return X\n",
    "\n",
    "\n",
    "def dsigmoid(X):\n",
    "    # TODO: implement the derivative of the sigmoid function, using the sigmoid function above\n",
    "    return X\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement `forward` and `forward_keep_all` functions for a model with a hidden layer with a sigmoid activation function:\n",
    "  - $\\mathbf{h} = sigmoid(\\mathbf{W}^h \\mathbf{x} + \\mathbf{b^h})$\n",
    "  - $\\mathbf{y} = softmax(\\mathbf{W}^o \\mathbf{h} + \\mathbf{b^o})$\n",
    "\n",
    "- Notes: \n",
    "  - try to keep the code as similar as possible as the previous one;\n",
    "  - `forward` now has a keep activations parameter to also return hidden activations and pre activations;\n",
    "\n",
    "- Update the grad function to compute all gradients; check that the gradients are well defined;\n",
    "\n",
    "- Implement the `train` and `loss` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "\n",
    "\n",
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # TODO: initialize the weights and biases with random numbers\n",
    "        self.W_h = None\n",
    "        self.b_h = None\n",
    "        self.W_o = None\n",
    "        self.b_o = None\n",
    "        self.output_size = output_size\n",
    "            \n",
    "    def forward_keep_activations(self, X):\n",
    "        # TODO: compute the forward pass and return the hidden activations and pre activations\n",
    "        z_h = 0.\n",
    "        h = 0.\n",
    "        y = np.zeros(size=self.output_size)\n",
    "        return y, h, z_h\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Calls the forward_keep_activations function and only returns the output\n",
    "        y, h, z_h = self.forward_keep_activations(X)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        # TODO: compute the average negative log likelihood over the whole training set\n",
    "        # You can use the nll function you implemented earlier\n",
    "        return 42.\n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO: compute the gradient of the loss with respect to W_h, b_h, W_o and b_o for a single sample (x, y_true)\n",
    "        return {\"W_h\": 0., \"b_h\": 0., \"W_o\": 0., \"b_o\": 0.}\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # TODO: compute the gradient for the sample and update the weights\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 10\n",
    "model = NeuralNet(n_features, n_hidden, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.accuracy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies, accuracies_test = [], [], []\n",
    "losses.append(model.loss(X_train, y_train))\n",
    "accuracies.append(model.accuracy(X_train, y_train))\n",
    "accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "print(\"Random init: train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "for epoch in range(15):\n",
    "    for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "        model.train(x, y, 0.1)\n",
    "\n",
    "    losses.append(model.loss(X_train, y_train))\n",
    "    accuracies.append(model.accuracy(X_train, y_train))\n",
    "    accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "    print(\"Epoch #%d, train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "          % (epoch + 1, losses[-1], accuracies[-1], accuracies_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Training loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Exercises\n",
    "\n",
    "### Look at worst prediction errors\n",
    "\n",
    "- Use numpy to find test samples for which the model made the worst predictions,\n",
    "- Use the `plot_prediction` to look at the model predictions on those,\n",
    "- Would you have done any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters settings\n",
    "\n",
    "- Experiment with different hyper-parameters:\n",
    "  - learning rate,\n",
    "  - size of hidden layer,\n",
    "  - initialization scheme: test with 0 initialization vs uniform,\n",
    "  - implement other activation functions,\n",
    "  - implement the support for a second hidden layer.\n",
    "\n",
    "\n",
    "### Back to Keras\n",
    "\n",
    "- Implement the same network architecture with Keras (look to Lab 1 for inspiration);\n",
    "\n",
    "- Check that the Keras model can approximately reproduce the behavior of the Numpy model when using similar hyperparameter values (size of the model, type of activations, learning rate value and use of momentum);\n",
    "\n",
    "- Compute the negative log likelihood of a sample 42 in the test set (can use `model.predict_proba`);\n",
    "\n",
    "- Compute the average negative log-likelihood on the full test set.\n",
    "\n",
    "- Compute the average negative log-likelihood  on the full training set and check that you can get the value of the loss reported by Keras.\n",
    "\n",
    "- Is the model overfitting or underfitting? (ensure that the model has fully converged by increasing the number of epochs to 50 or more if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
