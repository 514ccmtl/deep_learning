{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "\n",
    "In this lab, we'll be using Keras to build a recommender system. We'll be using the MovieLens dataset, a common benchmark dataset for recommender systems. \n",
    "\n",
    "MovieLens is a web-based recommender system and virtual community that recommends movies for its users to watch, based on their film preferences using collaborative filtering of members' movie ratings and movie reviews. You can check out the website here: https://movielens.org/\n",
    "\n",
    "We will download a subset of the dataset containing 100k ratings. There are tens of millions of ratings in the full dataset, spanning hundreds of thousands of users and movies. The subset we'll be using is a good example to demonstrate the concepts in this lab."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Check if the packages are installed, if not install them.\n",
    "# Note - if you are working locally, you may want to comment this section out\n",
    "# ...and use your preferred method of installing packages.\n",
    "import importlib\n",
    "\n",
    "def install_if_missing(package):\n",
    "    if importlib.util.find_spec(package) is None:\n",
    "        !pip install {package}\n",
    "\n",
    "for package in [\"matplotlib\", \"numpy\", \"sklearn\", \"pandas\", \"tensorflow\", \"plotly\"]:\n",
    "    install_if_missing(package)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "from pathlib import Path\n",
    "\n",
    "ML_100K_URL = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "ML_100K_FILENAME = Path(\"ml-100k.zip\")\n",
    "ML_100K_FOLDER = Path(\"ml-100k\")\n",
    "\n",
    "if not ML_100K_FOLDER.exists():\n",
    "    if not ML_100K_FILENAME.exists():\n",
    "        urlretrieve(ML_100K_URL, ML_100K_FILENAME.name)\n",
    "    with ZipFile(ML_100K_FILENAME.name) as zip:\n",
    "        zip.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MovieLens is made up of several datasets. The first one we will look at is `u.data`, which contains 100000 ratings by 943 users on 1682 items. Each user has rated at least 20 movies. Users and items are numbered consecutively from 1. The data is randomly ordered. This is a tab separated list of \n",
    "    - user id\n",
    "    - item id\n",
    "    - rating\n",
    "    - timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_ratings = pd.read_csv(ML_100K_FOLDER / \"u.data\", sep='\\t',\n",
    "                          names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "raw_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item metadata file\n",
    "\n",
    "The item metadata file contains metadata like the name of the movie or the date it was released. The movies file contains columns indicating the movie's genres. Let's only load the first five columns of the file with `usecols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['item_id', 'title', 'release_date', 'video_release_date', 'imdb_url']\n",
    "items = pd.read_csv(ML_100K_FOLDER / \"u.item\", sep='|', names=columns_to_keep,\n",
    "                    encoding='latin-1', usecols=range(5))\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the `release_date` column is a string containing the date in the format `dd-mm-yyyy`. We will convert it to a `datetime` object to make it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['release_date'] = pd.to_datetime(items['release_date']) # Pandas makes this easy!\n",
    "items['release_year'] = items['release_date'].dt.year # For later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to merge the ratings and the items metadata into a single dataframe. We will use the `item_id` column as the key to merge the two dataframes. This way, each row will contain all the information about a single rating: the user id, the item id, the rating, the timestamp, the title, the release date, the video release date and the IMDB url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings = pd.merge(items, raw_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "It's always important to understand the data you've collected. Thankfully, Pandas continues to make this easy for us. Using the `describe` method, we can get a quick statistical summary of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a bit more pandas magic to compute the popularity of each movie (number of ratings). We will use the `groupby` method to group the dataframe by the `item_id` column and then use the `size` method to compute the number of ratings for each movie. We will use the `reset_index` method to convert the resulting Series into a dataframe with an `item_id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity = all_ratings.groupby('item_id').size().reset_index(name='popularity')\n",
    "items = pd.merge(popularity, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['popularity'].plot.hist(bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(items['popularity'] == 1).sum() # Number of movies with only one rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.nlargest(10, 'popularity')['title'] # Get the 10 most popular movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings = pd.merge(popularity, all_ratings)\n",
    "all_ratings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "indexed_items = items.set_index('item_id')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later in the analysis we will assume that this popularity does not come from the ratings themselves but from an external metadata, e.g. box office numbers in the month after the release in movie theaters.\n",
    "\n",
    "Let's split the enriched data in a train / test split to make it possible to do predictive modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ratings_train, ratings_test = train_test_split(\n",
    "    all_ratings, test_size=0.2, random_state=0)\n",
    "\n",
    "user_id_train = np.array(ratings_train['user_id'])\n",
    "item_id_train = np.array(ratings_train['item_id'])\n",
    "rating_train = np.array(ratings_train['rating'])\n",
    "\n",
    "user_id_test = np.array(ratings_test['user_id'])\n",
    "item_id_test = np.array(ratings_test['item_id'])\n",
    "rating_test = np.array(ratings_test['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit feedback: supervised ratings prediction\n",
    "\n",
    "For each pair of (user, item) try to predict the rating the user would give to the item.\n",
    "\n",
    "This is the classical setup for building recommender systems from offline data with explicit supervision signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive ratings  as a regression problem\n",
    "\n",
    "The following code implements the following architecture:\n",
    "\n",
    "<img src=\"images/rec_archi_1.svg\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Dot\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample we input the integer identifiers\n",
    "# of a single user and a single item\n",
    "class RegressionModel(Model):\n",
    "    def __init__(self, embedding_size, max_user_id, max_item_id):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_user_id + 1,\n",
    "                                        input_length=1,\n",
    "                                        name='user_embedding')\n",
    "        self.item_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_item_id + 1,\n",
    "                                        input_length=1,\n",
    "                                        name='item_embedding')\n",
    "        \n",
    "        # The following two layers don't have parameters.\n",
    "        self.flatten = Flatten()\n",
    "        self.dot = Dot(axes=1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_inputs = inputs[0]\n",
    "        item_inputs = inputs[1]\n",
    "        \n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        item_vecs = self.flatten(self.item_embedding(item_inputs))\n",
    "        \n",
    "        y = self.dot([user_vecs, item_vecs])\n",
    "        return y\n",
    "\n",
    "\n",
    "model = RegressionModel(64, all_ratings['user_id'].max(), all_ratings['item_id'].max())\n",
    "model.compile(optimizer=\"adam\", loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for debugging the output shape of model\n",
    "initial_train_preds = model.predict([user_id_train, item_id_train])\n",
    "initial_train_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model error\n",
    "\n",
    "Using `initial_train_preds`, compute the model errors:\n",
    "- mean absolute error\n",
    "- mean squared error\n",
    "\n",
    "Converting a pandas Series to numpy array is usually implicit, but you may use `rating_train.values` to do so explicitly. Be sure to monitor the shapes of each object you deal with by using `object.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring runs\n",
    "\n",
    "Keras enables to monitor various variables during training. \n",
    "\n",
    "`history.history` returned by the `model.fit` function is a dictionary\n",
    "containing the `'loss'` and validation loss `'val_loss'` after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training the model\n",
    "history = model.fit([user_id_train, item_id_train], rating_train,\n",
    "                    batch_size=64, epochs=10, validation_split=0.1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.ylim(0, 2)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "- Why is the train loss higher than the first loss in the first few epochs?\n",
    "- Why is Keras not computing the train loss on the full training set at the end of each epoch as it does on the validation set?\n",
    "\n",
    "\n",
    "Now that the model is trained, the model MSE and MAE look nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y_true, y_pred):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.xlim(-1, 6)\n",
    "    plt.xlabel(\"True rating\")\n",
    "    plt.ylim(-1, 6)\n",
    "    plt.ylabel(\"Predicted rating\")\n",
    "    plt.scatter(y_true, y_pred, s=60, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "test_preds = model.predict([user_id_test, item_id_test])\n",
    "print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n",
    "print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))\n",
    "plot_predictions(rating_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict([user_id_train, item_id_train])\n",
    "print(\"Final train MSE: %0.3f\" % mean_squared_error(train_preds, rating_train))\n",
    "print(\"Final train MAE: %0.3f\" % mean_absolute_error(train_preds, rating_train))\n",
    "plot_predictions(rating_train, train_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Embeddings\n",
    "\n",
    "- It is possible to retrieve the embeddings by simply using the Keras function `model.get_weights` which returns all the model learnable parameters.\n",
    "- The weights are returned the same order as they were build in the model\n",
    "- What is the total number of parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and shape\n",
    "weights = model.get_weights()\n",
    "[w.shape for w in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = weights[0]\n",
    "item_embeddings = weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = 181\n",
    "print(f\"Title for item_id={item_id}: {indexed_items['title'][item_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Embedding vector for item_id={item_id}\")\n",
    "print(item_embeddings[item_id])\n",
    "print(\"shape:\", item_embeddings[item_id].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding most similar items\n",
    "\n",
    "Now that we have embeddings, we can compute the similarity between items in the embedding space. The cosine similarity is often used for that purpose:\n",
    "\n",
    "- Write in numpy a function to compute the cosine similarity between two points in embedding space.\n",
    "- Test it on the following cells to check the similarities between popular movies.\n",
    "\n",
    "Notes:\n",
    "- you may use `np.linalg.norm` to compute the norm of vector, and you may specify the `axis=`\n",
    "- the numpy function `np.argsort(...)` enables to compute the sorted indices of a vector\n",
    "- `items[\"name\"][idxs]` returns the names of the items indexed by array idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-07  # to avoid division by 0.\n",
    "\n",
    "\n",
    "def cosine(x, y):\n",
    "    # TODO: implement me!\n",
    "    return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similarity(item_a, item_b, item_embeddings, titles):\n",
    "    print(titles[item_a])\n",
    "    print(titles[item_b])\n",
    "    similarity = cosine(item_embeddings[item_a],\n",
    "                        item_embeddings[item_b])\n",
    "    print(f\"Cosine similarity: {similarity:.3}\")\n",
    "    \n",
    "print_similarity(50, 181, item_embeddings, indexed_items[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_similarity(181, 288, item_embeddings, indexed_items[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_similarity(181, 1, item_embeddings, indexed_items[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_similarity(181, 181, item_embeddings, indexed_items[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarities(item_id, item_embeddings):\n",
    "    \"\"\"Compute similarities between item_id and all items embeddings\"\"\"\n",
    "    query_vector = item_embeddings[item_id]\n",
    "    dot_products = item_embeddings @ query_vector\n",
    "\n",
    "    query_vector_norm = np.linalg.norm(query_vector)\n",
    "    all_item_norms = np.linalg.norm(item_embeddings, axis=1)\n",
    "    norm_products = query_vector_norm * all_item_norms\n",
    "    return dot_products / (norm_products + EPSILON)\n",
    "\n",
    "\n",
    "similarities = cosine_similarities(181, item_embeddings)\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(similarities, bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(item_id, item_embeddings, titles,\n",
    "                 top_n=30):\n",
    "    sims = cosine_similarities(item_id, item_embeddings)\n",
    "    # [::-1] makes it possible to reverse the order of a numpy\n",
    "    # array, this is required because most similar items have\n",
    "    # a larger cosine similarity value\n",
    "    sorted_indexes = np.argsort(sims)[::-1]\n",
    "    idxs = sorted_indexes[0:top_n]\n",
    "    return list(zip(idxs, titles[idxs], sims[idxs]))\n",
    "\n",
    "\n",
    "most_similar(50, item_embeddings, indexed_items[\"title\"], top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[items['title'].str.contains(\"Star Trek\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(227, item_embeddings, indexed_items[\"title\"], top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarities do not always make sense: the number of ratings is low and the embedding  does not automatically capture semantic relationships in that context. Better representations arise with higher number of ratings, and less overfitting  in models or maybe better loss function, such as those based on implicit feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing embeddings using TSNE\n",
    "\n",
    "The [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) algorithm enables to visualize high dimensional vectors in a 2D space by preserving local neighborhoods. We can use it to get a 2D visualization of the item embeddings and see if similar items are close in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "item_tsne = TSNE(learning_rate=\"auto\", init=\"pca\", perplexity=30).fit_transform(item_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(item_tsne[:, 0], item_tsne[:, 1]);\n",
    "plt.xticks(()); plt.yticks(());\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "tsne_df = pd.DataFrame(item_tsne, columns=[\"tsne_1\", \"tsne_2\"])\n",
    "tsne_df[\"item_id\"] = np.arange(item_tsne.shape[0])\n",
    "tsne_df = tsne_df.merge(items.reset_index())\n",
    "\n",
    "px.scatter(tsne_df, x=\"tsne_1\", y=\"tsne_2\",\n",
    "           color=\"popularity\",\n",
    "           hover_data=[\"item_id\", \"title\", \"popularity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    " - Add another layer to the neural network and retrain, compare train/test error.\n",
    " - Try adding more dropout and change layer sizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using item metadata in the model\n",
    "\n",
    "Using a similar framework as previously, we will build another deep model that can also leverage additional metadata. The resulting system is therefore an **Hybrid Recommender System** that does both **Collaborative Filtering** and **Content-based recommendations**.\n",
    "\n",
    "<img src=\"images/rec_archi_3.svg\" style=\"width: 600px;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "meta_columns = ['popularity', 'release_year']\n",
    "\n",
    "scaler = QuantileTransformer()\n",
    "item_meta_train = scaler.fit_transform(ratings_train[meta_columns])\n",
    "item_meta_test = scaler.transform(ratings_test[meta_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate\n",
    "\n",
    "class HybridModel(Model):\n",
    "\n",
    "    def __init__(self, embedding_size, max_user_id, max_item_id):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_user_id + 1,\n",
    "                                        input_length=1,\n",
    "                                        name='user_embedding')\n",
    "        self.item_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_item_id + 1,\n",
    "                                        input_length=1,\n",
    "                                        name='item_embedding')\n",
    "        \n",
    "        # The following two layers don't have parameters.\n",
    "        self.flatten = Flatten()\n",
    "        self.concat = Concatenate()\n",
    "        \n",
    "        self.dense1 = Dense(64, activation=\"relu\")\n",
    "        self.dropout = Dropout(0.3)\n",
    "        self.dense2 = Dense(64, activation='relu')\n",
    "        self.dense3 = Dense(1)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        user_inputs = inputs[0]\n",
    "        item_inputs = inputs[1]\n",
    "        meta_inputs = inputs[2]\n",
    "\n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        user_vecs = self.dropout(user_vecs, training=training)\n",
    "\n",
    "        item_vecs = self.flatten(self.item_embedding(item_inputs))\n",
    "        item_vecs = self.dropout(item_vecs, training=training)\n",
    "\n",
    "        input_vecs = self.concat([user_vecs, item_vecs, meta_inputs])\n",
    "\n",
    "        y = self.dense1(input_vecs)\n",
    "        y = self.dropout(y, training=training)\n",
    "        y = self.dense2(y)\n",
    "        y = self.dropout(y, training=training)\n",
    "        y = self.dense3(y)\n",
    "        return y\n",
    "        \n",
    "model = HybridModel(64, all_ratings['user_id'].max(), all_ratings['item_id'].max())\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "initial_train_preds = model.predict([user_id_train,\n",
    "                                     item_id_train,\n",
    "                                     item_meta_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit([user_id_train, item_id_train, item_meta_train],\n",
    "                    rating_train,\n",
    "                    batch_size=64, epochs=10, validation_split=0.1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict([user_id_test, item_id_test, item_meta_test])\n",
    "print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n",
    "print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional metadata seems to improve the predictive power of the model a bit but this should be re-run several times to see the impact of the random initialization of the model.\n",
    "\n",
    "\n",
    "### A recommendation function for a given user\n",
    "\n",
    "Once the model is trained, the system can be used to recommend a few items for a user that they haven't seen before. The following code does that.\n",
    "\n",
    "- we use the `model.predict` to compute the ratings a user would have given to all items\n",
    "- we build a function that sorts these items and excludes those the user has already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def recommend(user_id, top_n=10):\n",
    "    item_ids = range(1, items['item_id'].max())\n",
    "    seen_mask = all_ratings[\"user_id\"] == user_id\n",
    "    seen_movies = set(all_ratings[seen_mask][\"item_id\"])\n",
    "    item_ids = list(filter(lambda x: x not in seen_movies, item_ids))\n",
    "\n",
    "    print(\"User %d has seen %d movies, including:\" % (user_id, len(seen_movies)))\n",
    "    for title in all_ratings[seen_mask].nlargest(20, 'popularity')['title']:\n",
    "        print(\"   \", title)\n",
    "    print(\"Computing ratings for %d other movies:\" % len(item_ids))\n",
    "    \n",
    "    item_ids = np.array(item_ids)\n",
    "    user_ids = np.zeros_like(item_ids)\n",
    "    user_ids[:] = user_id\n",
    "    items_meta = scaler.transform(indexed_items[meta_columns].loc[item_ids])\n",
    "    \n",
    "    rating_preds = model.predict([user_ids, item_ids, items_meta])\n",
    "    \n",
    "    item_ids = np.argsort(rating_preds[:, 0])[::-1].tolist()\n",
    "    rec_items = item_ids[:top_n]\n",
    "    return [(items[\"title\"][movie], rating_preds[movie][0])\n",
    "            for movie in rec_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, pred_rating in recommend(5):\n",
    "    print(\"    %0.1f: %s\" % (pred_rating, title))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
